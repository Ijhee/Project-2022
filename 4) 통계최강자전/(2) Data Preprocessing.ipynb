{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bDjaCauesndz",
    "outputId": "b5931264-6901-44da-f812-2f13714734b2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHHFNwXdGPuR"
   },
   "source": [
    "## 1) Adding new Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMqpRanIGr3Q"
   },
   "source": [
    "### 1. Popularity Density \n",
    "#### : 1 if in main city else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ec7hPzYXG55h"
   },
   "outputs": [],
   "source": [
    "#Check index for main region code\n",
    "idx = df[df['Region_Code'] == 28].index\n",
    "\n",
    "r_lst = []\n",
    "for i in range(len(df)):\n",
    "    if i in idx:\n",
    "        r_lst.append('main')\n",
    "    else:\n",
    "        r_lst.append('notmain')\n",
    "\n",
    "df['Population'] = r_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr_C-qAZHYU4"
   },
   "source": [
    "### 2. Basic Insurance Customer\n",
    "##### : Groups with the lowest annual premium are treated as having only basic insurance. (Basic : 1, etc : 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1ftnWei7HWyo"
   },
   "outputs": [],
   "source": [
    "#Check index for basic annual premium\n",
    "idx = df[df['Annual_Premium'] == 2630].index\n",
    "\n",
    "a_lst = []\n",
    "for i in range(len(df)):\n",
    "    if i in idx: a_lst.append('basic')\n",
    "    else: a_lst.append('option')\n",
    "        \n",
    "df['Basic_Annual'] = a_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swtfHswtIMGe"
   },
   "source": [
    "### 3. Beneficiary of insurance\n",
    "#### : Pre-insured and treated as an insurance beneficiary if there is a history of car damage (Yes: 1, No: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9_Skliw0IYFk"
   },
   "outputs": [],
   "source": [
    "df['Beneficiary'] = np.where((df['Previously_Insured'] == 1) & (df['Vehicle_Damage'] == 'Yes'), 'benefit', 'not_benefit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h5Ut629I6Ln"
   },
   "source": [
    "### 4. Risk of an accident\n",
    "##### : If customer has a few years of experience in an accident, it is judged that its driving habit is dangerous\n",
    "##### -> High risk of accidents has a high need for insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QNwdpqoUJJti"
   },
   "outputs": [],
   "source": [
    "df['Danger'] = -1\n",
    "\n",
    "for i in range(len(df)):\n",
    "  #고위험군\n",
    "  if ((df.loc[i, 'Vehicle_Damage'] == 'Yes') & (df.loc[i, 'Vehicle_Age'] == '< 1 Year')):\n",
    "    df.loc[i, 'Danger'] = 'high'\n",
    "  #저위험군\n",
    "  elif ((df.loc[i, 'Vehicle_Damage'] == 'No') & (df.loc[i, 'Vehicle_Age'] == '> 2 Years')) == 1:\n",
    "    df.loc[i, 'Danger'] = 'low'\n",
    "  #그 외\n",
    "  else:\n",
    "    df.loc[i, 'Danger'] = 'mid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VmtnUd4KYWR"
   },
   "source": [
    "### 5. Risk of an accident2\n",
    "#### : Car accident while no insurance beneficienary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VCCTRGRTKimk"
   },
   "outputs": [],
   "source": [
    "df['N_Danger'] = np.where((df['Previously_Insured'] == 0) & (df['Vehicle_Damage'] == 'Yes'), 'high', 'low')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exZ8KXBAK3Kr"
   },
   "source": [
    "### 6. Percentage of accident experiences by age\n",
    "##### : Percentage of people who have experienced accidents by age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[df['Age']<= 25]\n",
    "r1 = round(df1['Vehicle_Damage'].value_counts()[1] / len(df1),4)\n",
    "\n",
    "df2=df[(df['Age']>=26) & (df['Age']<=36)]\n",
    "r2 = round(df2['Vehicle_Damage'].value_counts()[1] / len(df2),4)\n",
    "\n",
    "df3=df[(df['Age']>=37) & (df['Age']<=49)]\n",
    "r3 = round(df3['Vehicle_Damage'].value_counts()[0] / len(df3),4)\n",
    "\n",
    "df4=df[(df['Age']>=50) & (df['Age']<=85)]\n",
    "r4 = round(df4['Vehicle_Damage'].value_counts()[1] / len(df4),4)\n",
    "\n",
    "df['Age_damaged'] = -1\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i,'Age'] < 25:\n",
    "        df.loc[i,'Age_damaged'] = r1\n",
    "    elif (df.loc[i,'Age'] >= 26) & (df.loc[i,'Age'] <= 36):\n",
    "        df.loc[i,'Age_damaged'] = r2\n",
    "    elif (df.loc[i,'Age'] >= 37) & (df.loc[i,'Age'] <= 49):\n",
    "        df.loc[i,'Age_damaged'] = r3\n",
    "    else:\n",
    "        df.loc[i,'Age_damaged'] = r4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZItbvmrLIXY"
   },
   "source": [
    "### 7. Grouping Main Channel\n",
    "##### : Set the most frequent of each channel within the channel variable as the primary channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Oo5pMUljMOLI"
   },
   "outputs": [],
   "source": [
    "df['Main_Channel'] = -1\n",
    "for i in range(len(df)):\n",
    "    if (df.loc[i,'Policy_Sales_Channel'] == 152) | (df.loc[i,'Policy_Sales_Channel'] == 26) | (df.loc[i,'Policy_Sales_Channel'] == 124):\n",
    "        df.loc[i,'Main_Channel'] = 'main_ch'\n",
    "    else:\n",
    "        df.loc[i,'Main_Channel'] = 'notmain_ch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSOiuVeqMURG"
   },
   "source": [
    "### 8. Grouping Major channels by age group\n",
    "##### : Grouping since there is a channel with a high frequency by age group (by age 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "e97eS5saMFxq"
   },
   "outputs": [],
   "source": [
    "df['Age_Channel'] = 1000000\n",
    "for i in range(len(df)):\n",
    "    if (df.loc[i,'Policy_Sales_Channel'] == 26) | (df.loc[i,'Policy_Sales_Channel'] == 124):\n",
    "        df.loc[i,'Age_Channel'] = 'main_over'\n",
    "    elif (df.loc[i,'Policy_Sales_Channel'] == 152) | (df.loc[i,'Policy_Sales_Channel'] == 160):\n",
    "        df.loc[i,'Age_Channel'] = 'main_under'\n",
    "    else:\n",
    "        df.loc[i,\"Age_Channel\"] = 'channel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls7sOhEmMVnW"
   },
   "source": [
    "### 9. Age Group\n",
    "##### : Add age group variables because many characteristics are determined by age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_rT_2FW_Mh5z"
   },
   "outputs": [],
   "source": [
    "df['Age_group'] = 100000\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i,'Age'] <= 25:\n",
    "        df.loc[i,'Age_group'] = '~25'\n",
    "    elif (df.loc[i,'Age'] >= 26) & (df.loc[i,'Age'] <= 36):\n",
    "        df.loc[i,'Age_group'] = '26~36'\n",
    "    elif (df.loc[i,'Age'] >= 37) & (df.loc[i,'Age'] <= 49):\n",
    "        df.loc[i,'Age_group'] = '37~49'\n",
    "    else:\n",
    "        df.loc[i,'Age_group'] = '50~'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sReh0SOVMpCf"
   },
   "source": [
    "### 10. Young & Rich\n",
    "#### : When previously Insued and Response are '1', we guess they are young and rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xSny7aG_M4e8"
   },
   "outputs": [],
   "source": [
    "df['Young_Rich'] = 1000000\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if (df.loc[i,'Previously_Insured'] == 1) & (df.loc[i,'Response'] == 1) :\n",
    "        df.loc[i,'Young_Rich'] = 'Not_YR'\n",
    "    else:\n",
    "        df.loc[i,'Young_Rich'] = 'YR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Dataframe that adds the variables\n",
    "df.drop('id', axis = 1, inplace = True)\n",
    "df.to_csv('var_df.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ) Data Generation for Sub-Modeling\n",
    "##### Data is divided into two groups by Vehicle_age because their sub data have shown different distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 : Adding new variables X & submodeling X\n",
    "df = pd.read_csv('origin_df.csv')\n",
    "\n",
    "#2-1 : Adding new variables X & submodeling O\n",
    "model_b = df[df['Vehicle_Age'] == '> 2 Years']\n",
    "model_b.to_csv(\"origin_balance.csv\", index = False)\n",
    "\n",
    "#2-2 : Adding new variables X & submodeling O\n",
    "model_imb = df[df['Vehicle_Age'] != '> 2 Years']\n",
    "model_imb.to_csv(\"origin_imbalance.csv\", index = False)\n",
    "\n",
    "#3 : Adding new variables O & submodeling X\n",
    "var_df = pd.read_csv('var_df.csv')\n",
    "\n",
    "#3-1 : Adding new variables O & submodeling O\n",
    "model_imb = var_df[var_df['Vehicle_Age'] == '> 2 Years']\n",
    "model_imb.to_csv(\"var_balance.csv\", index = False)\n",
    "\n",
    "#3-2 : Adding new variables O & submodeling O\n",
    "model_imb = df1[df1['Vehicle_Age'] != '> 2 Years']\n",
    "model_imb.to_csv(\"var_imbalance.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data and maintain the origin ratio of target variables\n",
    "def data_split(df):\n",
    "\n",
    "    df_zero = df[df[\"target\"] == 0]\n",
    "    df_one = df[df[\"target\"] == 1]\n",
    "\n",
    "    X, y = df_zero.drop([\"target\"], axis = 1), df_zero[\"target\"]\n",
    "    a, b, c, d = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    X, y = df_one.drop([\"target\"], axis = 1), df_one[\"target\"]\n",
    "    e, f, g, h= train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "    temp1 = pd.concat([a, c], axis = 1)\n",
    "    temp2 = pd.concat([b, d], axis = 1)\n",
    "    temp3 = pd.concat([e, g], axis = 1)\n",
    "    temp4 = pd.concat([f, h], axis = 1)\n",
    "\n",
    "    df_train = pd.concat([temp1, temp3], axis = 0)\n",
    "    df_test = pd.concat([temp2, temp4], axis = 0)\n",
    "\n",
    "    df_train = df_train.sample(frac = 1)\n",
    "    df_test = df_test.sample(frac = 1)\n",
    "\n",
    "    df_train.reset_index(drop = True, inplace = True)\n",
    "    df_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for LabelEncoding\n",
    "def LabelEncoding(df):\n",
    "    \n",
    "    o_lst = df.select_dtypes('object').columns.tolist()\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    for col in o_lst:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df[col])\n",
    "        trs = le.transform(df[col])\n",
    "        df['encode_'+col] = trs\n",
    "    df = df.drop(o_lst, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for OneHotEncoding\n",
    "def OnehotEncoding(df):\n",
    "    \n",
    "    o_lst = df.select_dtypes('object').columns.tolist()\n",
    "    df = pd.get_dummies(data = df, columns = o_lst, prefix = 'encode', drop_first = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find errors in the data collection process and outliers that do not conform to common sense\n",
    "def outlier(df):\n",
    "    \n",
    "    ndf = df[df['Driving_License'] == 0]\n",
    "    idx = ndf[ndf['Vehicle_Damage'] == 'No'].index\n",
    "    df.drop(index=idx, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Train Data according to the number of cases of the preprocessing methodology\n",
    "def train_ver(df_tr, ver,name):\n",
    "    \n",
    "    if ver == 1:\n",
    "        ndf_train = outlier(df_tr)\n",
    "        ndf_train = LabelEncoding(ndf_train)\n",
    "        ndf_train.to_csv(name + \"1.csv\", index = False)\n",
    "    \n",
    "    elif ver == 2:\n",
    "        ndf_train = LabelEncoding(df_tr)\n",
    "        ndf_train.to_csv(name + \"2.csv\", index = False)\n",
    "\n",
    "    elif ver == 3:\n",
    "        ndf_train = outlier(df_tr)\n",
    "        ndf_train = OnehotEncoding(ndf_train)\n",
    "        ndf_train.to_csv(name + \"3.csv\", index = False)\n",
    "\n",
    "    else:\n",
    "        ndf_train = OnehotEncoding(df_tr)\n",
    "        ndf_train.to_csv(name + \"4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Test Data according to the number of cases of the preprocessing methodology\n",
    "def test_ver(df_test, ver,name):\n",
    "\n",
    "    if (ver == 1) | (ver == 2):\n",
    "        ndf_test = LabelEncoding(df_test)\n",
    "        ndf_test.to_csv(name + str(ver) + \".csv\", index = False)\n",
    "\n",
    "    elif (ver == 3) | (ver == 4):\n",
    "        ndf_test = OnehotEncoding(df_test)\n",
    "        ndf_test.to_csv(name + str(ver) + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP = [\"origin_imbalance.csv\", \"var_imbalance.csv\", \"origin_balance.csv\", \"var_balance.csv\", \n",
    "      \"origin_df.csv\", \"var_df.csv\"]\n",
    "\n",
    "def make_data(DATA_PATH):\n",
    "    \n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df_train, df_test = data_split(df)\n",
    "    name = list(DATA_PATH)[0]\n",
    "    _idx = [list(DATA_PATH).index(\".\"), list(DATA_PATH).index(\"_\")]\n",
    "    \n",
    "    if len(DATA_PATH) <= 13: name += _idx[0] - 1\n",
    "    else: name += _idx[1] + 1\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        df_tr, df_ts = df_train.copy(), df_test.copy()\n",
    "        train_ver(df_tr, i, name + \"_train\")\n",
    "        test_ver(df_ts, i, name + \"_test\")\n",
    "\n",
    "for DATA_PATH in DP:\n",
    "    make_data(DATA_PATH)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
